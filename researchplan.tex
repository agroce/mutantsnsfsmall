\section{Research Plan}

\subsection{Overview}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{TestFlow}

\caption{Basic flow of feedback-driven mutation testing.}
\label{fig:flow}
\end{figure}

Figure \ref{fig:flow} shows the basic outline of a proposed workflow
and components needed to support feedback-driven mutation testing.
These components serve to organize the research plan.


\subsubsection{FPF-Based Novelty Ranking}
\label{sec:fpfplan}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{distmetric}

\caption{Which mutants are most similar?  If the user marked the
  mutant in the upper left corner as uninteresting and added a test
  to kill the
  mutant in the upper middle, which mutant
  should she examine next?}
\label{fig:distances}
\end{figure}

Ranking unkilled mutants according to how much ``new'' information they might
provide to users requires more than simply using the FPF algorithm as
in fuzzer taming.  The key difference is the problem of determining
how similar two mutants are; in fuzzer taming, it is possible to
extract a large amount of information about similarity of failing
tests from executing the tests, and, in fact, from executing the tests
on program mutants~\cite{PLDI13,distMut}.  There is no obvious
equivalent to
``just run the test'' for mutants, and in fact avoiding the expense of running
the test suite on uninteresting mutants is one of the goals of
feedback-driven mutation testing in the first place.

FPF requires a distance metric, and a distance metric requires a
\emph{representation} of mutants.  Mutants can be similar because they
modify the same line, function, class, or module, but also because,
despite being located in very different parts of a program, they are
very semantically similar.  E.g., a mutant to the parser of a compiler, to
an I/O error-handling routine in the code generator, and to a complex
optimization pass may all be very ``similar'' in the only meaningful
sense if all three mutants modify logging statements that don't have
any actual effect on the state of the compiler.  Figure
\ref{fig:distances} shows the fundamental problem.  It is not,
a-priori, obvious which mutants here are most (dis-)similar.  Every
mutant has multiple plausible ``nearest neighbors'' --- another mutant
in the same file (likely to impact the same aspects of correctness),
another mutant with very similar code (likely to have the same kind of
semantic impact on the local context), or another mutant with the same operator (perhaps
likely to have some similarity, though probably of a lower importance
than the previous two types of similarity).  Are all logging statements
equivalent, or are only {\tt INFO} logging calls similar, while every
{\tt WARNING}, {\tt ERROR} or {\tt FATAL} is unique?  Some of these
decisions are unlikely to be project-independent, and so a good metric
may well change during feedback-driven mutation testing, in response
to information from users (see Section \ref{sec:feedbackplan} below).

Elements of the
distance metric obviously include, at minimum, mutant location, mutation operator, and
some representation of the code element modified --- language
construct, functions called, variables modified, and so forth.  These
static aspects may also be augmented with user feedback (as noted
above), but also with dynamic information obtained during the process,
such as frequency with which tests cover the mutated
statements/modules, or the way the mutant changes the
program path from the unmutated code in tests.  In fact, there may
need to be two distance metrics:  one for selecting likely candidate
mutants to execute, that uses only static information and user
feedback, and one that uses dynamic results from compiling and testing
mutants to refine the notion of similarity for likely-novel mutants.
This is therefore a quite complex problem in representation and weighting of elements of a
representation, especially for a language- and
project- agnostic metric, that is also open to tuning via feedback
analysis.  One approach to the problem is to exploit metric learning
methods~\cite{kulis2012metric}, which was used in some of PI Groce's
previous work~\cite{SoftMining}, but in order to avoid over-fitting
to even a set of good examples, the final metric may have to be largely
hand-tuned, and designed to incorporate feedback and dynamically
extracted information, which is not easily handled with learned metrics.  In part this is due to the difficulty of establishing
large amounts of ground truth data, and the expectation that
cross-project data will be less valuable than project-specific data
extracted during the process itself; although there are unsupervised
approaches to metric learning~\cite{scholkopf1998nonlinear,tipping1999probabilistic}, the most popular approaches require supervision.

\subsubsection{Mutant Utility Predictor}

Novelty with respect to previously analyzed mutants is not the only
important characteristic of a mutant.  Presenting a novel, but likely
equivalent mutant is often a waste of time, though some equivalent
mutants can be useful for identifying optimization opportunities or
refactorings.  Furthermore, of two similar mutants next to be presented,
it is better to present one that is higher in the mutant dominance
hierarchy (the one such that its tests will kill more other mutants).
There has been some initial work on predicting mutant quality
attributes and utility~\cite{MutQuality,FaRM}, including estimating how hard mutants
will be to kill, statically.  In addition to advancing the
state-of-the-art in that respect, feedback-driven mutation testing
also requires determining how to balance the need for novelty and the
predicted utility of a mutant.  For example, a utility-driven ranking
might suggest avoiding a highly novel mutant because it is likely
equivalent; however, it may be that labeling this mutant as equivalent
lets the FPF ranking avoid numerous other similar mutants --- e.g.,
postponing labeling a logging statement as confirmed equivalent by the
user may not be a
good idea.  Also, if the mutant is \emph{not} equivalent (perhaps the
user decides this kind of logging needs to be tested), then the
information obtained may be high-value.

\subsubsection{Feedback Analysis}
\label{sec:feedbackplan}

The ``feedback-driven'' aspect of feedback-driven mutation analysis
requires that information from the user be given high priority in the
process, a process with no clear equivalent in any previously proposed
mutation testing work.  The most straightforward example is that if a
user adds a test to kill a mutant, and marks that as a ``high impact''
action (the omitted testing was potentially allowing serious faults to
pass without detection) or even ``fault-revealing'' (the new test
detected a real fault in the system), then it may be most effective to
abandon the search for novelty and instead search for very similar
mutants still not killed by any test, in the expectation that these
may also result in high impact or fault-revealing tests.  If a user
marks a mutant as ``equivalent, but indicative of a refactoring
opportunity'', the same logic may apply:  similar mutants in other
parts of the code base may show the same problem with code quality,
even if they are predicted to be equivalent, and are not highly
novel.  In addition to informing the system of how useful various
analyzed mutants were, a user should also be able to inform the system
about correct and incorrect novelty rankings:  if the system presents
a mutant that is, from the user's POV, a (near-)duplicate of an
already handled mutant, the user should be able to express this fact,
and avoid future similar bad novelty estimates.

While large-scale crowdsourcing of user feedback is likely only
possible in some unusual industrial
settings~\cite{MutGoogle,ivankovic2018industrial}, it may also be
possible to apply mini-crowdsourcing techniques developed in the
context of testing machine-learning classifiers to mutant ranking and
analysis~\cite{Minicrowd}.  For high-visibility, high-criticality code
such as, e.g., Linux kernel modules, this may be a very powerful
tool.  The challenge in such a case is to allow communication between
feedback-driven mutation efforts, splitting work both so as to
minimize duplication and to target developers most familiar with
different aspects of the code base, as done in automated assignment of
bug reports~\cite{bhattacharya2012automated,jonsson2016automated}.

\subsubsection{Mutant Generator}
\label{sec:anylangplan}

The source of all mutants to be presented to the user is the mutant
generator, and in order to maximize the effectiveness of the approach,
this proposal aims to allow effective generation of mutants for any
programming 
language or DSL, with minimal additional effort.  The {\tt
  universalmutator} provides an initial source of mutants that
satisfies this requirement for initial experimentation, but for
long-term effectiveness is both inefficient and inexpressive.  The
current implementation avoids parsing to such an extent that it
generates numerous useless mutants embedded in code comments, or that
are obviously syntactically invalid.  While avoiding a parser-based
approach, simple additional constraints could avoid this, without
adding burden on users, such as allowing the definition of a
language's comment mechanisms, and not producing mutants inside
comments.  More generally, a mechanism for disabling mutation in
contexts defined in the same way as mutation operators would handle
other, even project-specific, constraints (e.g., never mutate inline
assembly in C/C++).  A problem with the current representation of
mutation operators and such contexts is that regular expressions are
currently applied only at the line level, and in any case are not
effective for defining such fundamentally non-regular aspects of code
as blocks and nested delimiters.  A key goal in this project is to
enhance the regular-expression language (without losing its simplicity
for ``normal'' operators) with the ability to express, in a
language-independent, syntactic form, such context-dependent aspects
of code.

{\bf CLG: COMBY HERE}

%the Cobra code-checking tool's language~\cite{Cobra} is an
%example of the kind of approach desired, though the language itself
%must be different, since transformation,
%not just detection, is the expressive content of a mutation operator.
Moreover, the generator will also need to be improved to allow users
to easily specify novel build environments and plug-ins for checking
Trivial Compiler Equivalence~\cite{TCE} to make the entire
feedback-driven mutation process workable.  Because some distance
metrics may require compiling mutants, which is costly, a specialized
projection of the distance only requiring textual analysis will need
to be developed, to allow generation, compilation, and execution of
only high-priority mutants for very large projects.

\subsection{Mutation-Driven-Development}

Rather than a separate research focus, the idea of
mutation-driven-development will inform the other
research topics.  In particular, once tools
reach sufficient maturity, they will be used to conduct preliminary
experiments in MDD as a methodology.  %In a sense, MDD is more central
%to the evaluation of this proposal's results than a separate research
%thrust, as discussed in the Evaluation Plan (Section \ref{sec:mevalplan}).

\subsection{Core Research Questions}

The component-focused sections above provide an overview of the
research problems to be addressed by this proposal, but it is also
useful to consider the high-level research questions to be addressed,
some of which are cross-cutting concerns independent of any single
component.

\subsubsection{Feedback-Driven Mutation Testing
Research Questions}

%\begin{framed}
\begin{enumerate}
\item What is a good generalized, language-agnostic mutant representation
  and distance metric?
\item How can FPF-based selection of mutants for novelty best incorporate
  predictions of mutant equivalence, outcome, dominance, and
  productivity?  Is novelty or expected utility more important?
\item How should feedback-driven mutation testing actually incorporate feedback from users into the 
  ranking of mutants?   What feedback should users be able to express,
  and how strongly should it be weighted?
\item Can mini-crowds be effectively leveraged to enhance the utility of user feedback?
\item Is it possible to identify outliers in otherwise similar groups of
  mutants, (e.g. one killed mutant in a cluser of unkilled mutants) and is such identification useful to users?
\item How can we quickly estimate whether a
  mutant is killable?
\item Is it possible to predict whether a mutant's unkillability is due to poor test 
  generation (hard to reach error states), oracle weakness
  (unidentified error states), or actual semantic equivalence? 
\item How can  we most effectively use already generated killing tests
  and counterexamples to prune mutants?
\item Is distance-based clustering plus timing information useful for quickly
  eliminating killable mutants similar to already-killed mutants?  How
  does this relate to Predictive Mutation Testing (PMT)?

\end{enumerate}
%\end{framed}

\subsubsection{Any-Language Mutation Research Questions}

%\begin{framed}
\begin{enumerate}
\item What advances are required in order to maximize the efficiency and usability of a
  fundamentally language-agnostic approach to
  mutant generation?
\item How should the language of regular expressions be extended to allow
  for language-agnostic definition of mutation operators that require
  more parser-like analysis of code structure, without compromising
  the usability and simplicity of the approach?
\item Is it possible to perform on-the-fly mutant generation for very
  large projects, and reconcile this approach with FPF (e.g., generate
  new mutants with, possibly approximate, desired distances from
  already evaluated mutants)?
\end{enumerate}
%\end{framed}

\input{workplan}
