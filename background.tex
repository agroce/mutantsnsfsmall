\section{Background and Preliminary Research}

\subsection{Falsification-Driven Verification and Testing}

PI Groce and colleagues recently proposed a novel approach to
formal verification and automated testing 
combining Karl Popper's falsification-based notion of
scientific discovery~\cite{Popper,popperconjectures} with mutation
testing~\cite{groce2015verified,groce2018verified,mutKernel}.
The heart of the idea is (1) the proposition that a surviving mutant that is not
equivalent (or at least equivalent with respect to a given
specification of correctness) \emph{falsifies} the claim that a given
formal verification or test effort captures the full notion of
correctness for a system and (2) the proposal that refining a
verification or test effort by repeated efforts at falsification is an
effective method for ensuring the quality of verification and testing
efforts.  This line of work
work resulted in the identification of multiple previously unknown faults in
the Linux kernel's
RCU~\cite{MathieuDesnoyers2012URCU,DinakarGuniguntala2008IBMSysJ,McKenney:2013:SDS:2483852.2483867}
module and the {\tt pyfakefs} Python mock file
system~\cite{pyfakefs}, despite the existence of very-high-quality
automated test generation efforts for these
systems~\cite{rcutorture,TSTL}.  This preliminary work on
falsification-driven methods proposed a number of algorithms and
methods for using unkilled mutants to guide model checking efforts,
estimate needed random testing budgets and loop-bounds in bounded model
checking~\cite{CBMCp,BMC}, and find bugs in testing and verification
harnesses, rather than the code under test.  At a high level, however,
the core concept was simple:  users should examine all unkilled
mutants of a program, and for each mutant either understand why it is
equivalent or uninteresting, or actually construct (with automated
help) a way to kill it.  In a sense, this simply harkens back to the
earliest ideas about mutation testing, but with the addition of
considerable automated support, at least for verification and
automated testing efforts.

Unfortunately, the
methods proposed were, while useful, limited in applicability.  These
early efforts simply assumed that the number of unkilled mutants to be examined was
small, and focused on solving the problem of helping a developer or
test engineer move from an unkilled mutant to an improvement of an
already very-high-quality
model-checking harness or  automated test generator.  Human
attention does not scale to analyzing large numbers of 
unkilled
mutants without further assistance in ``triaging'' the mutants,
however.  The process of manually examining mutants bears a
resemblance to the problem of manual confirmation of results from a
machine-learning classifier~\cite{OnlyOracle,EndUserMistake}, where even highly-motivated scientific
users are typically unwilling to examine more than a few tens of
potentially problematic results~\cite{Segal}.
The manual mutant-examination process, even aided by
tools for handling individual mutants, was simply not feasible unless the number of
unkilled mutants was relatively small, because the testing was already
very high quality.  Moreover, for a sufficiently large software system, even a
very high quality verification or testing effort may fail to kill a
large absolute number of mutants.  The original approach simply provided no way to
scale human efforts to such a needle-in-a-haystack setting. 

This project aims to make the falsification-driven approach to verification and testing feasible for larger
projects, or those with less effective testing or verification, and extremely easy for
smaller projects with already-high-quality testing or verification.
Moreover, because this proposal's feedback-driven approach no longer requires small absolute numbers of
unkilled mutants, it extends the applicability of the approach
to manual construction of tests to kill mutants, which was not usually in
scope when extremely high kill-rates were required.

\subsection{Furthest Point First and Fuzzer Taming}

An unkilled mutant is, conceptually, very similar to a failing test.
It presents information of possible relevance to a developer or test
engineer.  The mutant or test \emph{may} indicate the presence of a
previously unknown fault that needs to be fixed, either in the SUT or in the test suite/test
generator.  It may indicate the presence of a previously unknown fault
of less importance.  It may also indicate an even less interesting
result:  an equivalent mutant or 
a ``failure to fail'' of an inherently flaky test.  Or, in many cases, an unkilled
mutant or failing test may contain information that is either
important or unimportant, but is uninteresting because \emph{it
  duplicates information already presented for understanding.}  While
examining an equivalent mutant is not always useless (e.g., it may indicate
an opportunity for refactoring or improving the efficiency of code~\cite{ivankovic2018industrial,groce2018verified}), examining a mutant that is
equivalent to or extremely similar to an already-understood mutant is almost never
worthwhile --- even if the original mutant provided important,
actionable information.  That information has already been
incorporated into the development or testing process.

Fuzzer taming~\cite{PLDI13} was a solution PI Groce and colleagues proposed to the problem
of triaging test failures in automated test generation~\cite{SemCrash}.  In compiler
testing and other fuzzing applications, a core usability issue is that
tools tend to produce very large numbers of failing tests for a much
smaller number of distinct bugs.  Finding the set of distinct bugs,
and identifying important bugs that need to be fixed immediately is
difficult, because the important bugs may be represented by only one
or two failing tests in a set of thousands of failing tests, most of
which are duplicates.  The fuzzer taming work proposed that rather than highly imprecise
clustering, which does not work well in practice, and handles outliers
in a way that does not match the ``power law'' distribution of bugs, an
algorithm matching the goal of ranking maximally-different test
failures highly was appropriate.  Users do not (usually) care much
about finding the group of all tests failing due to a fault, or the
set of all mutants killable by the same extension to a test suite or
generator, but about seeing \emph{many very different test failures} or \emph{many
  different unkilled mutants} quickly, to maximize the chance of
discovering the most important faults or holes in a testing effort.
The \emph{furthest-point-first} (FPF) algorithm of
Gonzalez~\cite{Gonzalez85} does precisely this.  FPF, beginning with
any randomly chosen test (or mutant, in the present setting), always ranks
next the point in a metric-defined space that has the \emph{greatest
  distance from the previously ranked point to which it is closest.}
That is, for each point (test or mutant) not yet presented to the
user, FPF finds the closest among all already-ranked points, and
associates each unranked point with the distance to that closest
point.  The unranked point with the largest such distance is then
added to the ranking, and the process is repeated.  FPF can be
computed by a greedy algorithm, and is known to approximate novel-item
discovery for an optimal clustering~\cite{Gonzalez85}.  Preliminary work on the fuzzer taming problem using FPF-based
techniques~\cite{PLDI13,distMut} can be directly applied
to the similar (but operationally quite different) problem of ranking
unkilled mutants such that novel mutants are presented to a user.  The
research plan (Section
\ref{sec:fpfplan}), discusses the key \emph{differences} between novelty ranking
for mutants and
the fuzzer taming problem.

\subsection{Any-Language Regular-Expression Based Mutation}

As discussed in the problem statement, PI Groce and colleagues
released a functional, regular-expression-based mutant
generator~\cite{regexpMut,universalmutator}, and demonstrated that it
generated numbers of mutants and kill ratios for Java code comparable
to PIT~\cite{pittest} and Major~\cite{Major}, despite not parsing the
Java program at all, or acting at the bytecode level.  Furthermore,
for falsification-driven verification, the regular-expression-based
approach produced mutants of equal value to those produced by Andrews'
tool~\cite{mutant} and Muupi~\cite{muupi} for C and Python,
respectively.  As it stands, the {\tt universalmutator} tool is
usable for real-world mutation (in fact, it is being considered for
use by NASA/JPL engineers in testing the C code for upcoming CubeSat~\cite{CubeSat}
missions) for languages including: C, C++, Java, Python, Swift, Rust,
Go, and the Solidity smart-contract language.  The {\tt universalmutator} allows easy definition of new rules, and supports
automated analysis of mutants, coverage-based pruning of mutants, and
(in some languages) trivial compiler equivalence~\cite{TCE} checks.
As it stands, the tool and approach form a suitable platform for
generating mutants to be used in this project's early phases.  The
research plan (in Section \ref{sec:anylangplan}) discusses enhancements to the approach and tool that will be required
to better support robust, flexible, efficient feedback-driven mutation
testing.
