\section{Background and Preliminary Research}

\subsection{Falsification-Driven Verification and Testing}

PI Groce and colleagues recently proposed a novel approach to
formal verification and automated testing 
combining Karl Popper's falsification-based notion of
scientific discovery~\cite{Popper,popperconjectures} with mutation
testing~\cite{groce2015verified,groce2018verified,mutKernel}.
The heart of the idea is (1) a surviving non-equivalent mutant 
\emph{falsifies} the claim that a given
formal verification or test effort captures a full notion of
correctness and (2) refining a
verification or testing effort by repeated efforts at falsification is an
effective method for ensuring the quality of verification and testing
efforts.  This
work resulted in the identification of multiple previously unknown faults in
the Linux kernel's
RCU~\cite{MathieuDesnoyers2012URCU,DinakarGuniguntala2008IBMSysJ,McKenney:2013:SDS:2483852.2483867}
module and the {\tt pyfakefs} Python mock file
system~\cite{pyfakefs}, despite the existence of very-high-quality
automated test generation efforts for these
systems~\cite{rcutorture,TSTL}.  Out preliminary work on
falsification-driven methods proposed a number of algorithms and
methods for finding bugs in testing and verification
harnesses, rather than the code under test.  At a high level, however,
the core concept was simple:  users should examine all unkilled
mutants of a program, and for each mutant either understand why it is
equivalent or uninteresting, or actually construct a way to kill it.  In a sense, this actually harkens back to the
earliest ideas about mutation testing, but with 
much more automated support. 

Unfortunately, the
methods proposed were, while useful, limited in applicability.  They simply assumed that the number of unkilled mutants was
small, and focused on solving the problem of helping a developer or
test engineer move from an unkilled mutant to killing it.
In practice, however, human
attention does not scale to analyzing large numbers of 
unkilled mutants without further assistance in ``triaging'' the mutants.  The process of manually examining mutants bears a
resemblance to the problem of manual confirmation of results from a
machine-learning classifier~\cite{OnlyOracle,EndUserMistake}, where even highly-motivated scientific
users are typically unwilling to examine more than a few tens of
potentially problematic results~\cite{Segal}.
The manual mutant-examination process was simply not feasible unless the number of
unkilled mutants was relatively small, because the testing was already
very high quality.  For sufficiently large software systems, even a
very high quality testing effort may fail to kill a
large absolute number of mutants.  The original approach simply provided no way to
scale human efforts to such a needle-in-a-haystack setting. 

This project aims to make the falsification-driven approach to verification and testing feasible for larger
projects, and those with lower mutation scores.
Because this proposal's feedback-driven approach no longer requires small absolute numbers of
unkilled mutants, it extends the applicability of the approach
to using manual tests to kill mutants, which was not in
scope when extremely high kill-rates were required.

\subsection{Furthest Point First and Fuzzer Taming}

An unkilled mutant is, conceptually, very similar to a failing test.
It presents information of possible relevance to a developer.  The mutant or test \emph{may} indicate the presence of a
previously unknown fault that needs to be fixed, either in the SUT or in testing.  It may indicate the presence of a previously unknown fault
of less importance.  It may also indicate an even less interesting
result:  an equivalent mutant or 
an inherently flaky test.  Additionally, an unkilled
mutant or failing test may contain information that is uninteresting because \emph{it
  duplicates information already examined.}  While
examining an equivalent mutant is not always useless (e.g., it may indicate
an opportunity for refactoring or improving the efficiency of code~\cite{ivankovic2018industrial,groce2018verified}), examining a mutant that is
equivalent to, or extremely similar to, an already-understood mutant is almost never
worthwhile.  %That information has already been
%incorporated into the development or testing process.

Fuzzer taming~\cite{PLDI13} was a solution PI Groce and colleagues proposed to the problem
of triaging test failures in automated test generation~\cite{SemCrash}.  
Fuzzers tend to produce very large numbers of failing tests for a much
smaller number of distinct bugs.  Finding the set of distinct bugs,
and identifying important bugs that need to be fixed immediately is
difficult, because the important bugs may be represented by only one
or two failing tests in a set of thousands of failing tests, most of
which are duplicates.  The fuzzer taming work proposed that rather than highly imprecise
clustering, which does not work well in practice, and handles outliers
in a way that does not match the ``power law'' distribution of bugs, an
algorithm matching the goal of ranking maximally-different test
failures highly was appropriate.  Users do not (usually) care much
about finding the group of all tests failing due to a fault, or the
set of all mutants killable by the same extension to a test suite or
generator, but about seeing \emph{many very different test failures} or \emph{many
  different unkilled mutants} quickly, to maximize the chance of
discovering the most important faults or holes in a testing effort.
The \emph{furthest-point-first} (FPF) algorithm of
Gonzalez~\cite{Gonzalez85} does precisely this.  FPF, beginning with
any randomly chosen test (or mutant, in the present setting), always ranks
next the point in a metric-defined space that has the \emph{greatest
  distance from the previously ranked point to which it is closest.}
That is, for each point (test or mutant) not yet presented to the
user, FPF finds the closest among all already-ranked points, and
associates each unranked point with the distance to that closest
point.  The unranked point with the largest such distance is then
added to the ranking, and the process is repeated.  FPF can be
computed by a greedy algorithm, and is known to approximate novel-item
discovery for an optimal clustering~\cite{Gonzalez85}.  Preliminary work on the fuzzer taming problem using FPF-based
techniques~\cite{PLDI13,distMut} can be directly applied
to the different problem of ranking
unkilled mutants such that novel mutants are presented first.  %The
%research plan (Section
%\ref{sec:fpfplan}), discusses the key \emph{differences} between novelty ranking
%for mutants and
%the fuzzer taming problem.

\subsection{Any-Language Mutation}

PI Groce and colleagues
released a functional, regular-expression-based mutant
generator~\cite{regexpMut,universalmutator}, and demonstrated that it
generated numbers of mutants and kill ratios for Java code comparable
to PIT~\cite{pittest} and Major~\cite{Major}.  For falsification-driven verification, the regular-expression-based
approach produced mutants of equal value to those produced by Andrews'
tool~\cite{mutant} and Muupi~\cite{muupi} for C and Python,
respectively.  As it stands, the {\tt universalmutator} tool is
usable for real-world mutation (in fact, it is being considered for
use by NASA/JPL engineers in testing the C code for upcoming CubeSat~\cite{CubeSat}
missions) for languages including: C, C++, Java, Python, Swift, Rust,
Go, and the Solidity smart-contract language.  The {\tt universalmutator} allows easy definition of new rules, and supports
automated analysis of mutants, coverage-based pruning of mutants, and
(in some languages) trivial compiler equivalence~\cite{TCE} checks.
As it stands, the tool and approach form a suitable platform for
generating mutants to be used in this project's early phases.  The
research plan (in Section \ref{sec:anylangplan}) discusses enhancements to the approach and tool that will be required
to better support robust, flexible, efficient feedback-driven mutation
testing.

{\bf CLG: TALK ABOUT COMBY HERE.}

\subsection{Preliminary Use of FPF-Based Mutant Ranking}

In collaboration with security analysts at Trail of Bits, PI Groce
implemented a prototype version of mutant priorization, without
feedback, using a manually constructed distance metric tailored to
Solidity smart contracts.  This was an essential step in an effort to
use mutation analysis to compare three static analysis tools for smart
contracts~\cite{slitherpaper,sc:smartcheck,securify}.  The comparison of three such tools over 100 random contracts
from the Ethereum blockchain~\cite{buterin2013whitepaper,wood2014yellow} required analysis of 46,769 mutants, with
46,752 of these not killed by at least one static analysis tool.  The
aim of the analyis was to 1) find cases where mutation caused each
tool to flag a \emph{new} issue with the source code (thus
differentially identifying kills), then 2) find cases where at
least one tool flags a mutant, while others do not, and finally 3) baseline
this with respect to general warning rates for the tools over the same contracts.
Sorting through the surviving mutants to understand weaknesses of the
tools was simply not possible without using a simple version of the
FPF ranking algorithm, combined with an \emph{ad hoc} distance
metric.  With even this rudimentary, untuned version of our approach,
PI Groce and Trail of Bits engineers identified three significant new
detectors, which were implemented and added to the Slither static
analysis tool~\cite{slitherpaper}.  These are currently in use by Trail of Bits for
security audits, and will be released to the public after tuning.  Without the FPF ranking, finding these three
opportunities for improving Slither would not have been feasible.  To
our knowledge, the use of mutation to compare and improve static
analysis tools, rather than test suites, in this \emph{differential}
(both within tools and across tools) sense, is novel, and we believe that
feedback-driven approaches are essential in this setting, since kill
rates will obviously always be relatively low for purely static
analysis without a specification.  The preliminary results from the
Solidity mutation analysis are available at
\url{https://github.com/agroce/slithermutate}, with a publication to
follow once the same technique has been applied to other languages.






