The core problem this project aims to address is making the use of program mutants practical in non-research settings, in a way that meets developers' or test engineers' needs; that is, making it possible for someone creating or enhancing a test suite, or developing code and test suite in tandem, to (1) use ``just enough'' mutation testing for their needs, maximizing benefit gained in exchange for work performed, (2) work in any programming language without worrying about the quality of tool support provided for mutation testing, and without sacrificing the ease of understanding of source-based mutants, while easily adding custom mutation operators that target their specific software development task.  More generally, this project aims to make use of the insights of Test-Driven-Development (TDD), and proposes using mutation testing to move beyond a paradigm where developers build a series of tests narrowly tailored to steps in development, and use Mutation-Driven-Development (MDD) to build automated test generators or verification harnesses that handle not only anticipated problems imagined during development, but problems not anticipated by human insight, discovered using mutation-based analysis.  In addition to traditional manual testing, our approach targets both highly-general property-driven testing and even full formal verification of software components, in order to be practical in the future, where software systems will often be so safety- or mission- critical that even ``good'' manual testing is not an acceptable approach to ensuring correctness, security, and reliability.

\subsubsection{Just Enough Mutation Testing: Feedback-Driven Mutation Testing}

Most mutation testing work focuses on computing a mutation score or, at least, the set of unkilled (and perhaps non-equivalent) mutants.  Using mutation testing tools involves running many tests on many modified versions of a software system, and, for larger projects or more expensive test suites, requires substantial computing resources, making reducing that need a major thrust of mutation testing research~\cite{jia2011analysis}.  However, as useful as knowing the overall quality of a test suite may be, the most practical goal of mutation testing is to improve a test suite.  For this purpose, an expensive-to-generate list of all unkilled mutants is not really what users need or want.  A list of unkilled mutants contains uninteresting mutants (many, but not all, equivalent mutants), numerous redundant mutants (that can be killed by the same extension of the test suite, or rejected as uninteresting for the same reason), and a smaller number of actionable, representative mutants that are maximally effective in guiding improvement of a testing effort.  Examining all unkilled mutants is only practical for formal verification efforts or very high-powered test suites and critical software systems that motivate such efforts.  In our own work on using mutants to drive formal verification and automated testing~\cite{groce2015verified,groce2018verified,mutKernel} we note that examining surviving mutants was a time-consuming and unpleasant task, even in these settings.  With a larger number of unkilled mutants, the problem becomes one very much like the bug triage or ``fuzzer taming'' problem in random testing/fuzzing~\cite{PLDI13,SemCrash}:  a user wants to quickly find mutants that indicate the most important ``holes'' in a testing or verification effort, and act on those most-critical gaps, possibly revealing faults in the System Under Test (SUT).

What a user really wants is a tool that presents a few very different, ranked, mutants, all likely to be of interest, and revises the presented mutants and their ranking based on actions taken by the user --- adding tests, fixing faults, marking certain mutants as equivalent or uninteresting, and perhaps assigning a priority and severity to both killed or dismissed mutants and any remaining un-handled mutants. However, current mutation testing approaches make no real effort, with few exceptions~\cite{MutGoogle,FaRM} to prioritize mutants, and none are based on a user-centered feedback loop, where the user and mutation testing framework interact to improve a test suite, automated test generator, or verification harness --- and, of course, improve the SUT as well.  To our knowledge, in fact, other than (arguably) some efforts to incorporate dominance results~\cite{MutQuality}, no mutation testing approaches currently suggest any more sophisticated way to maximize the novelty of presented mutants than stratified sampling; stratified sampling does not really aim at semantic novelty, and can present many mutants from the same class together, if applied at the method level, even if those mutants are really quite similar in impact on the software system.

\begin{framed}
{\bf Problem:}  Develop highly automated methods and tools that allow the practical application of mutation testing in a feedback-driven way, where user and mutation testing framework cooperate to improve testing efforts, while minimizing user effort and maximizing the ability to quickly find the most important weaknesses of a test suite, automated test generation system, or formal verification effort.
\end{framed}

\subsubsection{Any-Language Mutation Testing}

One ongoing limitation of mutation testing is that tools are often research projects, and eventually become unusable due to lack of support, even in mainstream languages such as Java and C~\cite{MutChoice}.  This is because mutation tools that parse a language and guarantee generation of valid programs in the source language are complex, hard-to-maintain-and-extend systems; language complexity makes such a tool for C++, for example, an extremely daunting task.  The most widely used mutation tool in the real world, as far as we are aware, is PIT~\cite{pittest}, which targets Java bytecode.  There are recent attempts to provide the same kind of support for other languages, especially C, by targeting LLVM IR~\cite{HaririLLVM}.

The problem with bytecode-level mutation is that while an arguably excellent choice if the goal of mutation testing is to compute a score for a test suite, bytecode-level mutants are not nearly as suitable for presentation to users.  A bytecode-level mutation may not have a source-level equivalent that is conceptually simple, especially if the bytecode has been optimized, and some obvious source-level mutations (such as statement deletion) are known to be difficult to precisely implement in bytecode.  Moreover, targeting bytecode only helps with languages that compile to Java bytecode or LLVM IR, which leaves out Python, Ruby, Go, and numerous other popular languages, and certainly means that supporting project-specific Domain Specific Languages (DSLs)~\cite{Fow10} is out of the question.

\begin{figure}
\begin{tabularx}{0.75\textwidth}{XXX}
\verb|\+ ==> -| & \verb|== ==> !=| & \verb|(\D)(\d+)(\D) ==> \1(\2+1)\3|\\
\verb|\+ ==> *| & \verb|== ==> <| & \verb|(\D)(\d+)(\D) ==> \1(\2-1)\3|\\
\verb|\+ ==> /| & \verb|== ==> >| & \verb|(\D)(\d+)(\D) ==> \g<1>0\3|\\
\verb|".+" ==> ""| & \verb|while ==> if| & \verb|(^\s*)(\S+.*)\n ==> \1\2\n\1break;\n|\\
%{\tt \\+ ==> *} & {\tt != ==> <=} & {\tt (\D)(\\d+)(\D) ==> \\1\\2-1\\3}\\
%{\tt \\+ ==> /} & {\tt != ==> >=} & {\tt ".+" ==> ""}\\
%{\tt \\+ ==> \%} & {\tt != ==> >=} & {\tt (^\\s*)(\\S+.*)\\n ==> \\1\\2\\n\\1break;\\n}\\
\end{tabularx}
\caption{Some universal mutation rules}
\label{fig:rules}
\end{figure}

We recently proposed~\cite{regexpMut} an approach to mutant generation that does not attempt to parse source code, but simply defines mutation operators by a set of regular-expression-defined text transformations.  These are organzied into a hierarchy, so that if a program is, e.g., written in Swift, the ``universal'' mutation operators that apply to all programming languages are first applied, then operators for ``C-like'' languages, and finally a set of Swift-specific rules are applied.  Figure \ref{fig:rules} shows some of the current set of ``universal'' rules applied to all languages.  Adding a new language, even a custom DSL, or a new set of project-specific rules for an existing language, in this approach, simply requires writing a new rule file and defining where it lies in the language hierarchy.  This approach is also attractive in our feedback-based setting, since the problem of generating ``too many'' mutants is irrelevant if only a small set of highly diverse and likely-actionable mutants is ever presented to the user, and a novelty-estimator helps a user stop examining new mutants when the payoff is likely to be low.

There are significant limitations to this approach, however.  Because the source code is not parsed, and applies the regular expressions to lines of code, not larger blocks, the technique generates many mutants that are not valid programs, and cannot be compiled, or that are trivially equivalent because they, e.g., mutate ``source code'' in a large comment block.  Integrating mutation generation with execution is currently supported, but extending it to new languages or build systems is hard for users, requiring writing considerable Python code or complex shell scripts.  It is currently impossible to define mutation operators that apply to blocks of code rather than text within a single line, and standard regular expressions are not really suited to describing code constructs such as blocks, functions, classes, or structs.  Natural formatting of, e.g., an s-expression in a LISP-family language can hide opportunities for mutation, such as switching argument orders.

\begin{framed}
{\bf Problem:}  Provide high-quality mutation generation support to be used in a maximally flexible but still efficient mutation testing framework that can be applied to (essentially) any language, such that adding project-specific novel mutation operators, or even adding support for a new language (e.g. a DSL used in a single project) is possible for non-expert users.
\end{framed}

\subsubsection{Mutation-Driven-Development}

The primary focus of this project is to develop the algorithms and methods required for feedback-driven mutation testing, a process in which a user improves a verification or testing effort using a small number of well-selected mutants.  However, the ideas of Test-Driven Development (TDD)~\cite{TDD,TDDFuture}, which repeatedly turns requirements into specific test cases, then implements just enough functionality to pass the current tests (and assumes the previous implementation will not pass those tests), can be translated into a falsification-driven/mutation-driven form.  A potential weakness (and, of course, an actual goal) of TDD is that the code will be narrowly tailored to the requirements, which produce the tests, which means that missing requirements will almost always be omitted both from the tests and the code.  For ``shall'' type behaviors~\cite{INCOSE}, this is not a key problem; missing ``shall'' requirements will likely be omitted in any case.  But for security and safety, ``shall not'' requirements that are omitted can be deadly.  Mutation-Driven-Development (MDD) in its simplest form would require an application of feedback-driven mutation to the test suite at each development step or at least at major milestones, to ensure that code not only does what the tests require, but that the tests also sufficiently constrain the code to capture at least many implicit shall-nots.  Since such a process implemented by modifying TDD-driven tests would likely break the clean and appealing mapping between tests and requirements, and manual tests are inherently limited in effectiveness, for high-criticality systems, we suggest MDD should focus on augmenting TDD-driven tests with falsification-driven formal verification or at least automated testing.  One way to do this would be to ``elaborate'' TDD-produced unit tests into parameterized unit tests~\cite{UnitMeister,ParamUnit}, perhaps using a tool like DeepState~\cite{DeepState} for C/C++.  In such a process, weakness exposed by feedback-driven mutation testing would normally be addressed by taking an existing unit test and generalizing some parameters and assertions to kill the relevant mutants, letting AFL~\cite{aflfuzz}, libFuzzer~\cite{libfuzzer}, or a symbolic execution tool~\cite{angr1,angr2,manticore} identify specific inputs.  The focus of the MDD process would be on producing the test generator, test harness~\cite{WODACommon,tstlsttt}, or parameterized version of unit test that allows an automated tool to quickly kill mutants covered by additional specification.  More radically, MDD could be interpreted as a radical departure from normal TDD practice, where a single model checking or testing harness or parameterized unit test is iteratively enhanced with assertions and checks drawn from more requirements, always requiring the ability to kill (most) mutants of the current implementation that implements those requirements.  This does not produce the large set of tests in TDD, but instead produces a single, monolithic, high-powered test generator or formal specification.

\subsection{PI Qualifications}

PI Groce has been a user of, and contributor to, mutation testing tools for many years.  He combines a long research track record in software testing, including mutation testing, with actual experience testing critical software systems at NASA's Jet Propulsion Laboratory.  PI Groce's long-running interest in improving the state-of-the-art in mutation testing dates from frustration in his efforts to apply mutation tools to the testing and verification effort for the Mars Science Laboratory's flight software, in particular to the file system~\cite{ICSEDiff,CFV08,AMAI}.  This practical orientation informs recent work on using mutation testing in a falsification-driven approach to improving high-end verification and automated testing efforts~\cite{groce2015verified,groce2018verified,mutKernel}.  PI Groce has extensive experience in developing mutation tools for new languages~\cite{le2014mucheck,muupi,regexpMut}, including the first reliable tools for mutation of Haskell, Python, and Swift, as well as in user-facing (vs. researcher-oriented) automated software testing tools~\cite{tstlsttt,DeepState}.