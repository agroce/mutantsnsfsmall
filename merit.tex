This work addresses core problems not limited to practical application of mutation testing to improve test efforts in a user-centered way, but generalizeable to fundamental issues in software engineering and program semantics.  E.g., how can we represent program changes and (mostly statically) predict the similarity of their impact on program semantics, and which tests are likely to detect these changes.  How can novelty of information presented to a user be effectively balanced with a-priori predictions of the utility of that information, where likely-high-utility data points may also be unfortunately similar to each other?  How can user feedback be incorporated into such efforts without over-burdening human users?  This project also considers connections raised by our preliminary work, concerning effective methodologies for program and testing/verification effort development.  Can theoretical ideas about the nature of scientific discovery~\cite{Popper,popperconjectures,lakatos} be applied to such efforts?  Is falsification by alternative hypotheses about the correctness of a system or power of a testing/verification effort translateable to an actionable, effective approach for building systems~\cite{groce2015verified,groce2018verified}? 
The work on any-language mutation testing looks at syntactic patterns common to almost all programming languages, and relies on categorizing languages into families based on similarity, and how they share common meaningful syntactic changes that translate to interesting semantic changes.  