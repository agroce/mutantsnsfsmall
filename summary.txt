Overview:

The core problem this project aims to address is making the use of program mutants practical in non-research settings, in a way that meets developers' or test engineers' needs; that is, making it possible for someone creating or enhancing a test suite, or developing code and test suite in tandem, to (1) use ``just enough'' mutation testing for their needs, maximizing benefit gained in exchange for work performed, and (2) to work in any programming language without worrying about the quality of tool support provided for mutation testing, and without sacrificing the ease of understanding of source-based mutants, while easily adding custom mutation operators that target their specific software development task.  This project aims to adapt the Furthest-Point-First metric previously used in fuzzer bug triaging to the problem of maximizing the novelty of mutants examined by a user, in order to make it possible to quickly discover unkilled mutants that expose serious defects in a testing or verification effort.  However, novelty alone is not sufficient:  feedback-driven mutation testing must also help users avoid (uninteresting) equivalent mutants, kill mutants high in the dominance hierarchy, and (most importantly) incorporate user feedback.  If a user marks a mutant as uninteresting, or equivalent, or (especially) high impact, then that information must be used to inform the ranking of future mutants as well.  In order to make such an approach maximally valuable, this project also proposes to improve the state-of-the-art in regular-expression-based multilingual parser-free mutation generation, which allows users to easily generate mutants for new programming languages, or even for custom DSLs that are part of a specific project.

Intellectual Merit:

This project addresses core problems not limited to practical application of mutation testing to improve test efforts in a user-centered way, but generalizable to fundamental issues in software engineering and program semantics, e.g., how to represent program changes and (mostly statically) predict the similarity of their impact on program semantics, and predict which tests are likely to detect these changes.  How can novelty of information presented to a user be effectively balanced with a-priori predictions of the utility of that information, where likely-high-utility data points may also be unfortunately similar to each other?  How can user feedback be incorporated into such efforts without over-burdening human users?  This project also considers connections raised by preliminary work, concerning effective methodologies for program and testing/verification effort development.  Can theoretical ideas about the nature of scientific discovery be applied to such efforts?  Is falsification by alternative hypotheses about the correctness of a system or power of a testing/verification effort translatable to an actionable, effective approach for building systems?  The work on any-language mutation testing looks at syntactic patterns common to almost all programming languages, and relies on categorizing languages into families based on similarity, and how they share common meaningful syntactic changes that translate to interesting semantic changes. 
