\subsection{Work Plan}
\label{sec:workplan}


For this project, a simple plan should suffice, defined by components
of Figure \ref{fig:flow}.  In Year One, PI Groce and the graduate
student will both focus on the FPF-Based Novelty Ranking and
representation of mutants and test results.  This foundational work is
required before other parts of the approach can be applied.  For work
in the first two years, the existing mutant generation capabilities
will likely suffice, although it is possible that some improvements
will be discovered to provide enough benefit to merit early
implementation.  In Year
Two, both will focus on the Mutant Utility Predictor and Feedback
Analysis, and how to integrate those with the FPF-Based Novelty
Ranking.  In Year Three, the focus will shift to the Mutant Generator
itself, and the problem of any-language mutant generation.  By this
time, the demands made by the feedback-driven approach on the core
mutation engine will be much clearer, due to the relative maturity of
the user-facing components.  The third
year will also emphasize integrating the entire system and performing
more thorough evaluations.  It is likely that by the middle of Year
Two, a prototype system suitable for use by real developers and test
engineers not part of the project will be available, and the project
plan aims for the graduate student to work in an industrial or
government lab setting, and try applying the process to a real system,
during the summer of Year Two.

\subsection{Evaluation Plan}
\label{sec:evalplan}

Evaluation for this proposal includes both human-performed assessment
via trying to use feedback-driven mutation for actual test improvement
tasks and automated evaluations with more objective criteria, but a
weaker relationship to the actual goal of helping users quickly find
the most important unkilled mutants.  For the human portion, informal
evaluation will be performed by the research team itself, using known
programs with known testing weaknesses; this will help tune the
approach and experiment with new ideas.  However, for more advanced
assessment, expert users outside the team will be offered the chance
to use the system once it is in suitable shape.  In the past, PI Groce
has worked with IBM Distinguished Engineer Paul E. McKenney, a
prominent Linux kernel developers, on using mutants to improve kernel
test suites, and has discussed similar efforts with Richard Hipp, the
lead developer of the SQLite database, a rather famously well-tested
program, with some resulting improvements to both test suites.  Other
potential users with whom PI Groce has a working relationship include NASA/JPL engineers working on upcoming CubeSat
missions, colleagues working on the DeepState~\cite{DeepState}
parameterized unit testing interface to fuzzers and symbolic execution
engines, and the developers of {\tt pyfakefs}.  This type of
evaluation is, neccessarily, somewhat qualitative.  A retrospective
version can be applied by examining actual mutants that resulted in
improvements to the rcutorture~\cite{rcutorture} tests for the Linux
kernel and the {\tt pyfakefs} tests in previous
work~\cite{groce2018verified}, and comparing the useful mutants to the
highly ranked mutants:  could the highly ranked mutants have motivated
the key improvements to the test suites?

Just computing a mutant kill matrix for a good test suite can also
partially evaluate novelty rankings, if by ranking
killed, rather than unkilled mutants; the experiment even
realistically represents an early stage of test suite construction by
feedback-driven mutation testing, especially if the mutants are only
killed by a relatively small set of tests.  If
mutant X and mutant Y are both highly ranked, but killed by a very
similar set of tests, this indicates a possible problem with the
measure.  In real-world feedback-driven mutation testing, it is highly
desirable not to compute the full kill matrix for all mutants, but
for evaluation purposes, determining the extent to which kill
bitvector similarity agrees with FPF distance metric similarity serves
as a basic, if quite imperfect, sanity check on the novelty ranking.
Another automated way to evaluate a novelty ranking is to use automated
testing to generate multiple tests to kill each mutant in ranked order.  A good ranking will mean
that each additional mutant is unlikely to be killed by the killing
tests for any previous mutants.  A similar, but more robust, measure of mutant similarity
is how much adding using test to kill one mutant as the seed in a
fuzzer~\cite{aflfuzz,libfuzzer} improves time required to kill the other
mutant, on average.  Unfortunately, these measures cannot effectively
measure ``real distance'' if one of the mutants is equivalent.

Evaluation of the mutant generator can also be partly automated, by
comparing the output set of mutants to that of other tools (to ensure
no important mutants are omitted; the regular-expression based
approach will probably generate valid mutants not generated by other
tools, since it aims at a rich operator set, in accord with the
suggestions of multiple previous papers on detecting faults via
mutation~\cite{just2014mutants,gopinath2017mutation}.  Efficiency
gains (e.g., removal of invalid or equivalent-by-construction mutants) can
be measured by simple, traditional measures.

\subsubsection{Mutation-Driven-Development Evaluation}
\label{sec:mevalplan}

The evaluation of techniques and tools developed in this proposal will
also serve a dual purpose with respect to
Mutation-Driven-Development.  Using an MDD approach to implement
various small, but easy-to-get-wrong, code projects, such as binary
search and AVL trees, will make it possible both to see how effective
the tools for feedback-driven mutation testing are, and to see how effective an MDD approach to development is.  In
addition, using various versions of actual TDD efforts, with the
associated test suites for each step of development, it should be
possible to evaluate how much additional testing power MDD would have
required at each step of development.